{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Predicting Quality of Exercise using Personal Fitness Devices\"\nsubtitle: \"Practical Machine Learning: Prediction Assignment Writeup\"\nauthor: \"Beth Wolfset\"\ndate: \"September 10, 2016\"\noutput: \n  html_document: \n    keep_md: yes\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n# Executive Summary\n \nThe goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to quantify the manner in which they did the exercise. The \"classe\" variable in the training set shows the outcome as levels 'A', 'B', 'C', 'D', 'E'. The project will examine the other variables and determine the best model to predict the outcome.\n\nThis report describes the model, the method of cross validation, the expected out of sample error, and the reasons behind these choices. The models are used to predict 20 different test cases.\n\n# Background\n\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. Participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways.\n\nAdditional information: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).\n\n# Data\n\nThe training data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\n\nThe test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\n\nThe data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.\n\nThe following libraries are needed to reproduce the results:\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\n#install.packages(\"RCurl\", dependencies = TRUE)\nlibrary(RCurl)\n#install.packages(\"lubridate\", dependencies = TRUE)\nlibrary(lubridate)\n#install.packages(\"caret\", dependencies = TRUE)\nlibrary(caret)\n#install.packages(\"rattle\", dependencies = TRUE)\nlibrary(rattle)\n#install.packages(\"randomForest\", dependencies = TRUE)\nlibrary(randomForest)\n```\n\n### Load Data\n\nThe data is downloaded from the website.  A file will be written to the working directory.  It is then read into dataset variables.\n\n```{r, echo=TRUE}\n# Load the training dataset:\nTrainingURL <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\nTrainingFile <- \"pml-training.csv\"\ndownload.file(url=TrainingURL, destfile = TrainingFile)\nTrainingDS <- read.csv(TrainingFile, na.strings=c(\"NA\",\"\",\"#DIV/0!\"), header=TRUE)\nTrainingColNames <- colnames(TrainingDS)\n\n# Load test dataset:\nTestingURL <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\nTestingFile <- \"pml-testing.csv\"\ndownload.file(url=TestingURL, destfile = TestingFile)\nTestingDS <- read.csv(TestingFile, na.strings=c(\"NA\",\"\",\"#DIV/0!\"), header=TRUE)\nTestingColNames <- colnames(TestingDS)\n# Verify that the column names are equal in the training and test set.\nall.equal(TrainingColNames[1:length(TrainingColNames)-1], TestingColNames[1:length(TestingColNames)-1])\n```\n\nThis confirms that the number of columns in the Training and Testing sets is equal.  The exception is that the Training Data has the 'classe' column to use as the outcome.  The Testing set does not have a 'classe' column.  It does include a problem_id column that will be used for the submission.\n\n### Data Cleansing OF Training Data\n\nThe training data requires cleansing.  When a column has no values for a significant portion of the rows, then it cannot be used as a predictor.  I chose to remove columns where greater than 60% of the values contained 'NA'.  This has the affect of also removing columns with significant numbers of nulls.  As a result, there are no columns with near zero variables.\n\n```{r}\n# Remove columns with more than 60% NA\nTrainingDS <- TrainingDS[, !sapply(TrainingDS, function(y) (sum(is.na(y))/length(y)) > 0.60 )]\n\n# Make sure classe is a factor\nTrainingDS$classe <- as.factor(TrainingDS$classe)\n\n# Remove columns with near Zero Variables - not needed once mostly NA columns removed\n#Nr0Var <- nearZeroVar(TrainingDS)\n#if(length(Nr0Var) > 0) TrainDSNZV <- TrainingDS[, -Nr0Var]\n```\n\n### Split Training Data Set\nIt is important to determine how accurate the model is by determining the out of sample error.  The training set is large which allows it to be split for cross-validation.  In this way, the models will be trained on 65% of the training set and then used to predict the other 35% of the training set.\n\n```{r}\nset.seed(333)\ninTrain = createDataPartition(TrainingDS$classe, p=0.65)[[1]]\nMyTraining = TrainingDS[ inTrain,]\nMyTesting = TrainingDS[-inTrain,]\n\n# Remove first column so it cannot be used as predictor\nMyTraining <- MyTraining[c(-1)]\n```\n\n# Create the Predictive Models\n\nI have chosen to use two types of models to predict the quality of the exercises.  The first is the RPART model to generate a decision tree and look at its accuracy.  The seconds will be Random Forests.  In addition, I have used both the 'caret' package, and the specialized packages to train the models.\n\nThe method used is to:\n1. Train the model on the 65% subset of training data (MyTraining)\n\n2. Predict the outcome of the 35% subset of training data (MyTesting)\n\n3. Evaluate the accuracy of the model\n\n### Recursive Partitioning and Regression Trees (Rpart) model\n```{r}\n# RPART using caret package\nmdlrpart <- train(classe ~ ., data=MyTraining, method = \"rpart\")\n# take a look at the model\nprint(mdlrpart, digits=3)\n# take a better look at the decision tree\nfancyRpartPlot(mdlrpart$finalModel)\n# predict the MyTesting data and determine accuracy.\npredictrpart <- predict(mdlrpart, MyTesting )\nconfusionMatrix(predictrpart, MyTesting$classe)\n# RPART using rpart package\nmdlrpart2 <- rpart(classe ~ ., data=MyTraining, method = \"class\")\nprint(mdlrpart2, digits=3)\nfancyRpartPlot(mdlrpart2)\npredictrpart2 <- predict(mdlrpart2, MyTesting, type=\"class\" )\nconfusionMatrix(predictrpart2, MyTesting$classe)\n```\n\nThe decision tree model is easy to understand.  Using the 'caret' package the accuracy is only 53%. Using the 'rpart' package the accuracy is 86%.  Is is suspicious that the models from the two packages have such a large variance.  From the diagram of the decision tree, it is easy to see that the models themselves differ significantly.\n\n## Random Forests Model\n```{r}\n# using caret package\nmdlrf <- train(classe ~ ., data=MyTraining, method = \"rf\")\nprint(mdlrf, digits=3)\nprint(mdlrf$finalModel, digits=3)\npredictrf <- predict(mdlrf, MyTesting )\nconfusionMatrix(predictrf, MyTesting$classe)\n\n# using randomForest package\nmdlrf2 <- randomForest(classe ~ ., data=MyTraining)\nprint(mdlrf2, digits=3)\nprint(mdlrf2$finalModel, digits=3)\npredictrf2 <- predict(mdlrf2, MyTesting )\nconfusionMatrix(predictrf2, MyTesting$classe)\n```\n\nUsing the  Random Forests model from both the 'caret' and 'randomForest' package was able to predict the MyTesting set with .99% accuracy.  That is excellent accuracy which is suspicious.  It could be that the model has been overfit and there is not enough out of sample error to prove it wrong.\n\n# Preparing the Testing Submission\n\nNow that the models have been trained, they are used to predict the testing data.\n\n## Make Predictions on TestingDS\n```{r}\n# RPart from caret\npredictrpartTesting <- predict(mdlrpart, TestingDS )\nprint(predictrpartTesting)\n# RPart from rpart\npredictrpartTesting2 <- predict(mdlrpart2, TestingDS, type=\"class\"  )\nprint(predictrpartTesting2)\n# Random Forest from caret\npredictrfTesting <- predict(mdlrf, TestingDS )\nprint(predictrfTesting)\n# Random Forest from randomForest - since the random forest models are so similar, only used first to predict outcomes.\n#predictrfTesting2 <- predict(mdlrf2, TestingDS, type=\"class\" )\n#print(predictrfTesting2)\n```\n\n# Conclusions and Assumptions\n\nRandom Forest created a more accurate prediction model.  It took considerable more time to build this model and that will be an issue for large data sets.  The RPART model ran much faster but is also less accurate.\n\nThe expected out-of-sample error can be calculated as (1 - accuracy)\n\nRPART using caret had Accuracy .6134 with out of sample error .3866 or 39%\n\nRPART using rpart had Accuracy .8744 with out of sample error .1256 or 13%\n\nRandom Forest using caret had Accuracy .9996 with out of sample error .0004 or less than 1%\n\nRandom Forest using randomForest had Accuracy .9985 with out of sample error .0015 or less than 1%\n\n\nI used two different r packages to run the models.\nFor the RPARTS model, the accuracy of the models generated differed significantly.  Similarly, the  predictions of the test data were quite different.\nFor the Random Forests models, both packages yielded models with similar accuracy.",
    "created" : 1474550193615.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "77729952",
    "id" : "7D619722",
    "lastKnownWriteTime" : 1474550560,
    "last_content_update" : 1474550560029,
    "path" : "~/GitHub/Users/bwolfset/Documents/GitHub/PracticalMachineLearn/Users/bwolfset/Documents/GitHub/index.Rmd",
    "project_path" : "index.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}